---
title: "Project 3: Webscraping the Phnom Penh Post with rvest"
author: "Andrew S. Roberts"
version: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

**Webscraping the Phnom Penh Post with rvest**

18 May 2023

Goal:

The aim of this exercise was to learn how to use rvest to scrape data from webpages, assemble these data into a table, and then export the data as a CSV for analysis elsewhere.

I chose to work with the Phnom Penh Post because it is an important English language newspaper for Cambodia. It also was an opportunity for me to begin to explore English language news from Cambodia regarding forced labor at sea. I ultimately scraped data from the 4 pages which result from a site search using the terms "slavery" and "fishing." I collected the URL, title, date of publication and the full text of each article. 

I will analyze the data I collected for this project, or expand the search to include additional search terms, in a follow up project.

I roughly followed the tutorial at: 

https://tm4ss.github.io/docs/Tutorial_1_Web_scraping.html

However, I also drew widely from more recent tutorials, stack overflow as well as additional documentation when necessary (i.e. often!).

I walk through the procedure piece by piece (scraping the body of the text, title, date), bringing these all together at the end with a function block and a for loop.

*Scraping a Single Article*

1. Set working directory
2. Install required packages (tidyverse, rvest)

```{r, include = FALSE}
setwd("E:/GIS_DATA/PROJECTS/Project_3_webscraping")
options(stringsAsFactors = F)
install.packages("tidyverse",repos = "http://cran.us.r-project.org")
install.packages("rvest",repos = "http://cran.us.r-project.org")
install.packages("xml2",repos = "http://cran.us.r-project.org")
library(tidyverse)
library(rvest)
library(xml2)

```


3. To define the article URL and scrape the article text:

Article URL: https://phnompenhpost.com/national/khmers-escape-slavery-sea

```{r}
url <- "https://phnompenhpost.com/national/khmers-escape-slavery-sea"
html_document <- read_html(url)
text_xpath <- "//div[contains(@id, 'ArticleBody')]"
article_text <- html_document %>%
  html_node(xpath = text_xpath) %>%
  html_text (trim = TRUE)
#cat(article_text)

```

4. To scrape the article title:

```{r}
title_xpath <- "/html/body/div[3]/div/div[5]/h2"
article_title <- html_document %>%
  html_element(xpath = title_xpath) %>%
  html_text(trim = T)
#cat(article_title)
```

6. To scrape the article date, and format it to ISO 8601 standard:

```{r}
article_date_string <- html_document %>%
  html_elements ("span[itemprop='datePublished']") %>%
  html_text(trim = TRUE)
#print (article_date_string)
article_date <- substr(article_date_string, start = 6, nchar(article_date_string))
date <- dmy(article_date)
article_date_8601 <- format(date, "%Y-%m-%d")
#print(article_date_8601)
```

7. Put URL, title, date and text into a data frame (for later export into a csv):

```{r}
article_table <- data.frame (
    date = article_date_8601,
    title = article_title,
    url = url,
    text = article_text
)
#print(article_table)
```
*Webscraping multiple articles from the Phnom Penh Post website* 

Using web site search terms "slavery" and "fishing" on the Phnom Penh Post website yields 4 pages of articles.

Goal: Scrape the article date, title, URL and text for each link listed on the 4 pages of search results. Then assemble these in a data frame and export as a CSV for later analysis.

Base URL: https://phnompenhpost.com/search/node/slavery%20fishing

1. Scrape all twenty article links from the first search page of search results:

```{r}
url <- "https://phnompenhpost.com/search/node/slavery%20fishing"
html_document <- read_html(url)
links <- html_document %>%
  html_elements (xpath = "/html/body/div[3]/div/div[2]/div/ol/li/h3/a") %>%
  html_attr(name = "href")
#print(links)
```
2. Define paging URLs to collect links from all 4 pages of search results, combining the base URL with the pagination suffix for the additional pages:

```{r}
page_numbers <- 0:3
base_url <- "https://phnompenhpost.com/search/node/slavery%20fishing?page="
paging_urls <- paste0(base_url, page_numbers)
#print(paging_urls)
```
3. To scrape article links from each of the search result pages using a For-Loop:

```{r}
all_links <- NULL
for (url in paging_urls) {

  html_document <- read_html(url)
links <- html_document %>%
  html_elements (xpath = "/html/body/div[3]/div/div[2]/div/ol/li/h3/a") %>%
  html_attr(name = "href")
  
# append links to vector of all links
  all_links <- c(all_links, links)
}
#print(all_links)
```

4. Prepare function block to encapsulate the collection of url, title, date and article text for a single url into a data frame (as above, but all in one step). This is not looped. But the function block can be plugged in anywhere I need to scrape the data from the page (for example, looped below).

```{r}
scrape_PPP_article <- function(url) {
  
  #load document and scrape text
  url <- "https://phnompenhpost.com/national/khmers-escape-slavery-sea"
  html_document <- read_html(url)
  text_xpath <- "//div[contains(@id, 'ArticleBody')]"
  article_text <- html_document %>%
  html_node(xpath = text_xpath) %>%
  html_text (trim = TRUE)
 
 #scrape title  
  title_xpath <- "/html/body/div[3]/div/div[5]/h2"
 article_title <- html_document %>%
  html_element(xpath = title_xpath) %>%
  html_text(trim = T)
 
 #scrape date 
  article_date_string <- html_document %>%
  html_elements ("span[itemprop='datePublished']") %>%
  html_text(trim = TRUE)
  article_date <- substr(article_date_string, start = 6, nchar(article_date_string))
  date <- dmy(article_date)
  article_date_8601 <- format(date, "%Y-%m-%d")
 
  #append title, url, date and full text into a data frame 
  article_table <- data.frame (
    date = article_date_8601,
    title = article_title,
    url = url,
    text = article_text
  )
  
  return(article_table)
  
}

```
5. Use the function block above (scrape_PPP_article) to loop over all of the URLs in the scraped list of URLs.This block also gives instructions to print out the progress of the task.

```{r}
#prints progress of process
all_articles_table <- data.frame()
for (i in 1:length(all_links)) {
  cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
  article <- scrape_PPP_article(all_links[i])
  # Append current article data.frame to the data.frame of all articles
  all_articles <- rbind(all_articles_table, article_table)
}
```

To stitch it all together into a usable chunk of code...

```{r}
#Define paging URLs
page_numbers <- 0:3
base_url <- "https://phnompenhpost.com/search/node/slavery%20fishing?page="
paging_urls <- paste0(base_url, page_numbers)
#print(paging_urls)

#Scrape all links for pages specified in paging URL
all_links <- NULL
for (url in paging_urls) {

  html_document <- read_html(url)
links <- html_document %>%
  html_elements (xpath = "/html/body/div[3]/div/div[2]/div/ol/li/h3/a") %>%
  html_attr(name = "href")
  
# append links to vector of all links
  all_links <- c(all_links, links)
}

scrape_PPP_article <- function(url) {
  
  #load document and scrape text
  html_document <- read_html(url)
  text_xpath <- "//div[contains(@id, 'ArticleBody')]"
  article_text <- html_document %>%
  html_node(xpath = text_xpath) %>%
  html_text (trim = TRUE)
 
 #scrape title  
  title_xpath <- "/html/body/div[3]/div/div[5]/h2"
  article_title <- html_document %>%
  html_element(xpath = title_xpath) %>%
  html_text(trim = T)
 
 #scrape date 
  article_date_string <- html_document %>%
  html_elements ("span[itemprop='datePublished']") %>%
  html_text(trim = TRUE)
  article_date <- substr(article_date_string, start = 6, nchar(article_date_string))
  date <- dmy(article_date)
  article_date_8601 <- format(date, "%Y-%m-%d")
 
  #append title, url, date and full text into a data frame 
  article_table <- data.frame (
    date = article_date_8601,
    title = article_title,
    url = url,
    text = article_text
  )
  
  return(article_table)
  
}
#prints progress of process
all_articles_table <- data.frame()
for (i in 1:length(all_links)) {
  cat("Downloading", i, "of", length(all_links), "URL:", all_links[i], "\n")
  article_table <- scrape_PPP_article(all_links[i])
  # Append current article data.frame to the data.frame of all articles
  all_articles_table <- rbind(all_articles_table, article_table)
}

#write to csv
write.csv2(all_articles_table, file = "E:/GIS_DATA/PROJECTS/Project_3_webscraping/PPP_slavery_fishing_2023-05-19.csv")

```




